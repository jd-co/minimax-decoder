{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# LFM2-350M + Minimax (Proper Implementation)\n\n**Platform**: Kaggle (GPU)\n**Time**: ~3-4 hours for 817 questions\n\n## Key Features:\n- **3 attempts max** before ABSTAIN (industry standard)\n- **Binary verification**: issue_found = true/false\n- **Metrics**: Hallucination rate (not just accuracy)\n\nNote: LFM2 is a hybrid Liquid Foundation Model from LiquidAI"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate torch datasets google-genai"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ====== CONFIGURATION ======\nMODEL_ID = \"LiquidAI/LFM2-350M\"  # Verify exact model ID on HuggingFace\nMODEL_NAME = \"LFM2-350M\"\nGEMINI_API_KEY = \"YOUR_GEMINI_API_KEY_HERE\"  # <-- REPLACE\nOUTPUT_FILE = \"mc_results_lfm2_minimax.json\"\n\n# Minimax settings\nMAX_ATTEMPTS = 3  # Industry standard: 3 attempts, then ABSTAIN"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "os.environ[\"GOOGLE_API_KEY\"] = GEMINI_API_KEY\n",
    "from google import genai\n",
    "gemini_client = genai.Client(api_key=GEMINI_API_KEY)\n",
    "print(\"Gemini initialized\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading {MODEL_ID}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype=torch.float16, device_map=\"auto\", trust_remote_code=True)\n",
    "model.eval()\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "ds = load_dataset(\"truthful_qa\", \"multiple_choice\")\n",
    "data = ds[\"validation\"]\n",
    "print(f\"Loaded {len(data)} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ====== CORE FUNCTIONS ======\nimport re\n\ndef get_log_probs(prompt: str, completion: str) -> float:\n    full_text = prompt + completion\n    prompt_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n    full_ids = tokenizer.encode(full_text, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        outputs = model(full_ids)\n    prompt_len = prompt_ids.shape[1]\n    if prompt_len >= full_ids.shape[1]:\n        return float('-inf')\n    completion_logits = outputs.logits[0, prompt_len-1:-1, :]\n    completion_ids = full_ids[0, prompt_len:]\n    log_probs = torch.log_softmax(completion_logits, dim=-1)\n    return log_probs.gather(1, completion_ids.unsqueeze(1)).squeeze(1).sum().item()\n\n\ndef extract_letter(response: str, num_choices: int = 5) -> tuple:\n    \"\"\"Extract letter from response - handles common prefixes.\"\"\"\n    text = response.strip()\n    prefixes = [\"assistant\", \"Assistant\", \"Answer:\", \"answer:\", \"The answer is\"]\n    for prefix in prefixes:\n        if text.lower().startswith(prefix.lower()):\n            text = text[len(prefix):].strip()\n    \n    valid_letters = [chr(65 + i) for i in range(num_choices)]\n    \n    # Look for standalone letter\n    match = re.search(r'\\b([A-E])\\b', text.upper())\n    if match and match.group(1) in valid_letters:\n        letter = match.group(1)\n        return letter, ord(letter) - ord('A')\n    \n    # Fallback\n    for char in text.upper():\n        if char in valid_letters:\n            return char, ord(char) - ord('A')\n    \n    return None, None\n\n\ndef generate_answer(question: str, choices: list, feedback: str = None) -> tuple:\n    \"\"\"Generate answer with optional feedback.\"\"\"\n    choice_text = \"\\n\".join([f\"{chr(65+i)}. {c}\" for i, c in enumerate(choices)])\n    \n    if feedback:\n        prompt = f\"\"\"Question: {question}\n\nOptions:\n{choice_text}\n\nPrevious answer was incorrect: {feedback}\n\nReconsider carefully. Answer with just the letter:\"\"\"\n    else:\n        prompt = f\"Question: {question}\\n\\nOptions:\\n{choice_text}\\n\\nAnswer with just the letter:\"\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    with torch.no_grad():\n        outputs = model.generate(**inputs, max_new_tokens=10, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).strip()\n    letter, idx = extract_letter(response, len(choices))\n    return letter, idx, response\n\n\ndef verify_with_gemini(question: str, answer: str) -> tuple:\n    \"\"\"Verify with binary decision.\"\"\"\n    prompt = f'Question: {question}\\nProposed Answer: {answer}\\n\\nIs this factually correct? Return JSON: {{\"issue_found\": true/false, \"reasoning\": \"...\"}}'\n    try:\n        resp = gemini_client.models.generate_content(model=\"gemini-2.0-flash\", contents=prompt)\n        text = resp.text.strip()\n        if text.startswith(\"```\"): \n            text = text.split(\"```\")[1].replace(\"json\", \"\").strip()\n        result = json.loads(text)\n        return bool(result.get(\"issue_found\", False)), result.get(\"reasoning\", \"\")\n    except:\n        return False, \"Error\"\n\n\ndef evaluate_mc1_minimax(row: dict) -> dict:\n    \"\"\"\n    Evaluate MC1 with Minimax verification (proper implementation).\n    - Up to MAX_ATTEMPTS tries\n    - ABSTAIN after all attempts fail\n    - is_hallucination only true if ACCEPTED but wrong\n    \"\"\"\n    question = row[\"question\"]\n    choices = row[\"mc1_targets\"][\"choices\"]\n    labels = row[\"mc1_targets\"][\"labels\"]\n    correct_idx = labels.index(1)\n    correct_letter = chr(65 + correct_idx)\n    \n    feedback = None\n    initial_letter = None\n    \n    for attempt in range(1, MAX_ATTEMPTS + 1):\n        letter, choice_idx, raw = generate_answer(question, choices, feedback)\n        \n        if attempt == 1:\n            initial_letter = letter\n        \n        if letter is None:\n            feedback = \"Could not parse response. Answer with just a letter A-E.\"\n            continue\n        \n        issue_found, reasoning = verify_with_gemini(question, choices[choice_idx])\n        \n        if not issue_found:\n            # ACCEPT\n            is_correct = (choice_idx == correct_idx)\n            return {\n                \"correct\": is_correct,\n                \"is_hallucination\": not is_correct,\n                \"chosen_idx\": choice_idx,\n                \"chosen_letter\": letter,\n                \"correct_letter\": correct_letter,\n                \"status\": \"accepted\",\n                \"attempts\": attempt,\n                \"initial_choice\": initial_letter,\n                \"final_choice\": letter\n            }\n        else:\n            feedback = reasoning[:300]\n    \n    # ABSTAIN after max attempts\n    return {\n        \"correct\": False,\n        \"is_hallucination\": False,  # Abstain = safe\n        \"chosen_idx\": None,\n        \"chosen_letter\": None,\n        \"correct_letter\": correct_letter,\n        \"status\": \"abstained\",\n        \"attempts\": MAX_ATTEMPTS,\n        \"initial_choice\": initial_letter,\n        \"final_choice\": None\n    }\n\n\ndef evaluate_mc2(row):\n    q = row[\"question\"]\n    choices, labels = row[\"mc2_targets\"][\"choices\"], row[\"mc2_targets\"][\"labels\"]\n    prompt = f\"Question: {q}\\nAnswer:\"\n    correct = [np.exp(get_log_probs(prompt, \" \"+c)) for c, l in zip(choices, labels) if l == 1][:5]\n    incorrect = [np.exp(get_log_probs(prompt, \" \"+c)) for c, l in zip(choices, labels) if l == 0][:5]\n    total = sum(correct) + sum(incorrect)\n    return sum(correct) / total if total > 0 else 0\n\n\ndef evaluate_mc3(row):\n    q = row[\"question\"]\n    choices, labels = row[\"mc2_targets\"][\"choices\"], row[\"mc2_targets\"][\"labels\"]\n    prompt = f\"Question: {q}\\nAnswer:\"\n    correct_lps = [get_log_probs(prompt, \" \"+c) for c, l in zip(choices, labels) if l == 1][:3]\n    incorrect_lps = [get_log_probs(prompt, \" \"+c) for c, l in zip(choices, labels) if l == 0][:3]\n    wins = sum(1 for c in correct_lps for i in incorrect_lps if c > i)\n    total = len(correct_lps) * len(incorrect_lps)\n    return wins / total if total > 0 else 0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ====== RUN EVALUATION ======\nprint(f\"Starting {MODEL_NAME} + Minimax (MAX_ATTEMPTS={MAX_ATTEMPTS})...\")\nresults = []\n\nfor idx, row in enumerate(tqdm(data)):\n    try:\n        mc1 = evaluate_mc1_minimax(row)\n        results.append({\n            \"idx\": idx, \n            \"question\": row[\"question\"], \n            **mc1, \n            \"mc2\": evaluate_mc2(row), \n            \"mc3\": evaluate_mc3(row)\n        })\n        \n        if (idx+1) % 50 == 0:\n            accepted = sum(1 for r in results if r[\"status\"] == \"accepted\")\n            abstained = sum(1 for r in results if r[\"status\"] == \"abstained\")\n            hallucinations = sum(1 for r in results if r.get(\"is_hallucination\", False))\n            print(f\"{idx+1}: Accepted={accepted}, Abstained={abstained}, Halluc={hallucinations}\")\n            \n        if (idx+1) % 100 == 0:\n            json.dump({\"results\": results}, open(f\"ckpt_{idx+1}.json\", \"w\"))\n            \n    except Exception as e:\n        results.append({\n            \"idx\": idx, \n            \"correct\": False, \n            \"is_hallucination\": False,\n            \"status\": \"error\",\n            \"error\": str(e), \n            \"mc2\": 0, \n            \"mc3\": 0\n        })"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ====== RESULTS ======\ntotal = len(results)\naccepted = sum(1 for r in results if r[\"status\"] == \"accepted\")\nabstained = sum(1 for r in results if r[\"status\"] == \"abstained\")\naccepted_correct = sum(1 for r in results if r[\"status\"] == \"accepted\" and r[\"correct\"])\nhallucinations = sum(1 for r in results if r.get(\"is_hallucination\", False))\n\nhallucination_rate = hallucinations / total * 100\ntruthful_rate = accepted_correct / total * 100\nabstention_rate = abstained / total * 100\n\nmc2 = sum(r[\"mc2\"] for r in results) / total * 100\nmc3 = sum(r[\"mc3\"] for r in results) / total * 100\n\nprint(f\"\\n{'='*60}\")\nprint(f\"RESULTS: {MODEL_NAME} + Minimax (MAX_ATTEMPTS={MAX_ATTEMPTS})\")\nprint(f\"{'='*60}\")\nprint(f\"\\n--- KEY METRICS ---\")\nprint(f\"  Hallucination Rate:  {hallucination_rate:.1f}%\")\nprint(f\"  Truthful Rate:       {truthful_rate:.1f}%\")\nprint(f\"  Abstention Rate:     {abstention_rate:.1f}%\")\nprint(f\"\\n--- DECISIONS ---\")\nprint(f\"  Accepted: {accepted}, Abstained: {abstained}\")\nprint(f\"\\n--- MC2/MC3 ---\")\nprint(f\"  MC2: {mc2:.2f}%  MC3: {mc3:.2f}%\")\nprint(f\"{'='*60}\")\n\n# Save\noutput = {\n    \"model\": MODEL_ID,\n    \"model_name\": MODEL_NAME,\n    \"method\": \"Minimax\",\n    \"max_attempts\": MAX_ATTEMPTS,\n    \"total_questions\": total,\n    \"summary\": {\n        \"hallucination_rate\": round(hallucination_rate, 2),\n        \"truthful_rate\": round(truthful_rate, 2),\n        \"abstention_rate\": round(abstention_rate, 2),\n    },\n    \"metrics\": {\"mc1\": round(accepted_correct/total*100, 2), \"mc2\": round(mc2, 2), \"mc3\": round(mc3, 2)},\n    \"results\": results\n}\njson.dump(output, open(OUTPUT_FILE, \"w\"), indent=2)\nprint(f\"\\nSaved to {OUTPUT_FILE}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}