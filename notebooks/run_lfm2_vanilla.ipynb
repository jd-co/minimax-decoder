{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# LFM2-350M Vanilla - Baseline\n\n**Platform**: Kaggle (GPU)\n**Time**: ~1 hour for 817 questions\n\n**Purpose**: Baseline comparison for LFM2 + Minimax."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "!pip install -q transformers accelerate torch datasets"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "MODEL_ID = \"LiquidAI/LFM2-350M\"\nMODEL_NAME = \"LFM2-350M\"\nOUTPUT_FILE = \"mc_results_lfm2_vanilla.json\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, torch, numpy as np\nfrom tqdm import tqdm\nfrom datetime import datetime\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom datasets import load_dataset\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading {MODEL_ID}...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID, trust_remote_code=True)\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype=torch.float16, device_map=\"auto\", trust_remote_code=True)\nmodel.eval()\nprint(\"Loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "data = load_dataset(\"truthful_qa\", \"multiple_choice\")[\"validation\"]\nprint(f\"{len(data)} questions\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def get_log_probs(prompt, completion):\n    full = prompt + completion\n    p_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n    f_ids = tokenizer.encode(full, return_tensors=\"pt\").to(device)\n    with torch.no_grad(): logits = model(f_ids).logits\n    p_len = p_ids.shape[1]\n    if p_len >= f_ids.shape[1]: return float('-inf')\n    c_logits = logits[0, p_len-1:-1, :]\n    c_ids = f_ids[0, p_len:]\n    lp = torch.log_softmax(c_logits, dim=-1)\n    return lp.gather(1, c_ids.unsqueeze(1)).squeeze(1).sum().item()\n\ndef extract_letter(resp, n=5):\n    for c in resp.upper():\n        if c in [chr(65+i) for i in range(n)]: return c, ord(c)-65\n    return 'A', 0\n\ndef eval_mc1(row):\n    q = row[\"question\"]\n    choices, labels = row[\"mc1_targets\"][\"choices\"], row[\"mc1_targets\"][\"labels\"]\n    correct_idx = labels.index(1)\n    prompt = f\"Question: {q}\\n\\nOptions:\\n\" + \"\\n\".join([f\"{chr(65+i)}. {c}\" for i,c in enumerate(choices)]) + \"\\n\\nAnswer:\"\n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    with torch.no_grad(): out = model.generate(**inputs, max_new_tokens=10, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n    resp = tokenizer.decode(out[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).strip()\n    _, idx = extract_letter(resp, len(choices))\n    return {\"correct\": idx == correct_idx, \"chosen_idx\": idx, \"raw\": resp}\n\ndef eval_mc2(row):\n    q, mc2 = row[\"question\"], row[\"mc2_targets\"]\n    prompt = f\"Question: {q}\\nAnswer:\"\n    correct = [np.exp(get_log_probs(prompt, \" \"+c)) for c,l in zip(mc2[\"choices\"], mc2[\"labels\"]) if l==1][:5]\n    incorrect = [np.exp(get_log_probs(prompt, \" \"+c)) for c,l in zip(mc2[\"choices\"], mc2[\"labels\"]) if l==0][:5]\n    t = sum(correct) + sum(incorrect)\n    return sum(correct)/t if t > 0 else 0\n\ndef eval_mc3(row):\n    q, mc2 = row[\"question\"], row[\"mc2_targets\"]\n    prompt = f\"Question: {q}\\nAnswer:\"\n    c_lps = [get_log_probs(prompt, \" \"+c) for c,l in zip(mc2[\"choices\"], mc2[\"labels\"]) if l==1][:3]\n    i_lps = [get_log_probs(prompt, \" \"+c) for c,l in zip(mc2[\"choices\"], mc2[\"labels\"]) if l==0][:3]\n    wins = sum(1 for c in c_lps for i in i_lps if c > i)\n    t = len(c_lps) * len(i_lps)\n    return wins/t if t > 0 else 0"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "results = []\nfor idx, row in enumerate(tqdm(data)):\n    try:\n        mc1 = eval_mc1(row)\n        results.append({\"idx\": idx, **mc1, \"mc2\": eval_mc2(row), \"mc3\": eval_mc3(row)})\n        if (idx+1) % 50 == 0: print(f\"{idx+1}: {sum(r['correct'] for r in results)/len(results):.3f}\")\n    except Exception as e:\n        results.append({\"idx\": idx, \"correct\": False, \"mc2\": 0, \"mc3\": 0})"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ====== RESULTS ======\ntotal = len(results)\ncorrect_count = sum(r[\"correct\"] for r in results)\nwrong_count = total - correct_count\n\nhallucination_rate = wrong_count / total * 100\ntruthful_rate = correct_count / total * 100\n\nmc1 = truthful_rate\nmc2 = sum(r[\"mc2\"] for r in results) / total * 100\nmc3 = sum(r[\"mc3\"] for r in results) / total * 100\n\nprint(\"\\n\" + \"=\"*60)\nprint(f\"RESULTS: {MODEL_NAME} Vanilla (No Verification)\")\nprint(\"=\"*60)\nprint(f\"\\n--- KEY METRICS ---\")\nprint(f\"  Hallucination Rate:  {hallucination_rate:.1f}%\")\nprint(f\"  Truthful Rate:       {truthful_rate:.1f}%\")\nprint(f\"\\n--- MC SCORES ---\")\nprint(f\"  MC1: {mc1:.2f}%  MC2: {mc2:.2f}%  MC3: {mc3:.2f}%\")\nprint(\"=\"*60)\n\noutput = {\n    \"model\": MODEL_ID,\n    \"model_name\": MODEL_NAME,\n    \"method\": \"Vanilla\",\n    \"total_questions\": total,\n    \"summary\": {\n        \"hallucination_rate\": round(hallucination_rate, 2),\n        \"truthful_rate\": round(truthful_rate, 2),\n        \"abstention_rate\": 0.0,\n    },\n    \"metrics\": {\"mc1\": round(mc1, 2), \"mc2\": round(mc2, 2), \"mc3\": round(mc3, 2)},\n    \"results\": results\n}\njson.dump(output, open(OUTPUT_FILE, \"w\"), indent=2)\nprint(f\"\\nSaved to {OUTPUT_FILE}\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}