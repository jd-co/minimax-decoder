{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# SmolLM2-360M Vanilla (No Minimax) - Baseline\n\n**Platform**: Kaggle/Colab (GPU)\n**Time**: ~1 hour for 817 questions\n\n**Purpose**: Baseline comparison for Minimax. Every wrong answer = hallucination (no verification)."
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install -q transformers accelerate torch datasets google-genai"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ====== CONFIGURATION ======\nMODEL_ID = \"HuggingFaceTB/SmolLM2-360M-Instruct\"\nMODEL_NAME = \"SmolLM2-360M\"\nOUTPUT_FILE = \"mc_results_smollm2_vanilla.json\"\nGEMINI_API_KEY = \"YOUR_GEMINI_API_KEY_HERE\"  # <-- REPLACE THIS"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json\n",
    "import torch\n",
    "import numpy as np\n",
    "from tqdm import tqdm\n",
    "from datetime import datetime\n",
    "from transformers import AutoModelForCausalLM, AutoTokenizer\n",
    "from datasets import load_dataset\n",
    "\n",
    "device = \"cuda\" if torch.cuda.is_available() else \"cpu\"\n",
    "print(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Initialize Gemini for letter extraction\nimport os\nos.environ[\"GOOGLE_API_KEY\"] = GEMINI_API_KEY\nfrom google import genai\ngemini_client = genai.Client(api_key=GEMINI_API_KEY)\nprint(\"Gemini initialized for letter extraction\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading {MODEL_ID}...\")\n",
    "tokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\n",
    "model = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype=torch.float16, device_map=\"auto\")\n",
    "model.eval()\n",
    "print(\"Model loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ====== CORE FUNCTIONS ======\n\ndef get_log_probs(prompt: str, completion: str) -> float:\n    full_text = prompt + completion\n    prompt_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n    full_ids = tokenizer.encode(full_text, return_tensors=\"pt\").to(device)\n    \n    with torch.no_grad():\n        outputs = model(full_ids)\n        logits = outputs.logits\n    \n    prompt_len = prompt_ids.shape[1]\n    if prompt_len >= full_ids.shape[1]:\n        return float('-inf')\n    \n    completion_logits = logits[0, prompt_len-1:-1, :]\n    completion_ids = full_ids[0, prompt_len:]\n    log_probs = torch.log_softmax(completion_logits, dim=-1)\n    token_log_probs = log_probs.gather(1, completion_ids.unsqueeze(1)).squeeze(1)\n    return token_log_probs.sum().item()\n\n\ndef extract_letter_with_gemini(response: str, choices: list) -> tuple:\n    \"\"\"Use Gemini to extract which letter the model chose.\"\"\"\n    choice_text = \"\\n\".join([f\"{chr(65+i)}. {c}\" for i, c in enumerate(choices)])\n    \n    prompt = f\"\"\"The model was asked to pick an answer from these options:\n{choice_text}\n\nThe model responded: \"{response}\"\n\nWhich option (A, B, C, D, E, or F) did the model choose? \nReturn ONLY a JSON object:\n{{\"letter\": \"A\" or \"B\" or \"C\" etc}}\"\"\"\n\n    try:\n        resp = gemini_client.models.generate_content(\n            model=\"gemini-2.0-flash\",\n            contents=prompt\n        )\n        text = resp.text.strip()\n        \n        if text.startswith(\"```\"):\n            text = text.split(\"```\")[1]\n            if text.startswith(\"json\"):\n                text = text[4:]\n        text = text.strip()\n        \n        result = json.loads(text)\n        letter = result.get(\"letter\", \"A\").upper()\n        if letter in [chr(65+i) for i in range(len(choices))]:\n            return letter, ord(letter) - 65\n        return \"A\", 0\n    except:\n        return \"A\", 0\n\n\ndef evaluate_mc1_vanilla(row: dict) -> dict:\n    question = row[\"question\"]\n    mc1 = row[\"mc1_targets\"]\n    choices = mc1[\"choices\"]\n    labels = mc1[\"labels\"]\n    \n    correct_idx = labels.index(1)\n    correct_letter = chr(65 + correct_idx)\n    correct_answer = choices[correct_idx]\n    \n    choice_text = \"\\n\".join([f\"{chr(65+i)}. {c}\" for i, c in enumerate(choices)])\n    prompt = f\"Question: {question}\\n\\nOptions:\\n{choice_text}\\n\\nAnswer:\"\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    \n    with torch.no_grad():\n        outputs = model.generate(inputs.input_ids, max_new_tokens=10, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n    \n    response = tokenizer.decode(outputs[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).strip()\n    letter, idx = extract_letter_with_gemini(response, choices)\n    \n    return {\n        \"correct\": idx == correct_idx,\n        \"chosen_idx\": idx,\n        \"chosen_letter\": letter,\n        \"chosen_answer\": choices[idx],\n        \"correct_idx\": correct_idx,\n        \"correct_letter\": correct_letter,\n        \"correct_answer\": correct_answer,\n        \"raw_response\": response\n    }\n\n\ndef evaluate_mc2(row: dict) -> float:\n    question = row[\"question\"]\n    mc2 = row[\"mc2_targets\"]\n    choices = mc2[\"choices\"]\n    labels = mc2[\"labels\"]\n    \n    prompt = f\"Question: {question}\\nAnswer:\"\n    correct = [c for c, l in zip(choices, labels) if l == 1]\n    incorrect = [c for c, l in zip(choices, labels) if l == 0]\n    \n    correct_probs = [np.exp(get_log_probs(prompt, \" \" + a)) for a in correct[:5]]\n    incorrect_probs = [np.exp(get_log_probs(prompt, \" \" + a)) for a in incorrect[:5]]\n    \n    total = sum(correct_probs) + sum(incorrect_probs)\n    return sum(correct_probs) / total if total > 0 else 0.0\n\n\ndef evaluate_mc3(row: dict) -> float:\n    question = row[\"question\"]\n    mc2 = row[\"mc2_targets\"]\n    choices = mc2[\"choices\"]\n    labels = mc2[\"labels\"]\n    \n    prompt = f\"Question: {question}\\nAnswer:\"\n    correct = [c for c, l in zip(choices, labels) if l == 1]\n    incorrect = [c for c, l in zip(choices, labels) if l == 0]\n    \n    correct_lps = [get_log_probs(prompt, \" \" + a) for a in correct[:3]]\n    incorrect_lps = [get_log_probs(prompt, \" \" + a) for a in incorrect[:3]]\n    \n    wins = sum(1 for c in correct_lps for i in incorrect_lps if c > i)\n    total = len(correct_lps) * len(incorrect_lps)\n    return wins / total if total > 0 else 0.0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Starting {MODEL_NAME} Vanilla...\")\n",
    "print(f\"Start: {datetime.now()}\")\n",
    "\n",
    "results = []\n",
    "for idx, row in enumerate(tqdm(data)):\n",
    "    try:\n",
    "        mc1 = evaluate_mc1_vanilla(row)\n",
    "        mc2 = evaluate_mc2(row)\n",
    "        mc3 = evaluate_mc3(row)\n",
    "        results.append({\"question_idx\": idx, \"question\": row[\"question\"], **mc1, \"mc2_score\": mc2, \"mc3_score\": mc3})\n",
    "        \n",
    "        if (idx + 1) % 50 == 0:\n",
    "            acc = sum(r[\"correct\"] for r in results) / len(results)\n",
    "            print(f\"Progress {idx+1}/{len(data)}: MC1={acc:.3f}\")\n",
    "        if (idx + 1) % 100 == 0:\n",
    "            with open(f\"checkpoint_{idx+1}.json\", \"w\") as f:\n",
    "                json.dump({\"results\": results}, f)\n",
    "    except Exception as e:\n",
    "        print(f\"Error at {idx}: {e}\")\n",
    "        results.append({\"question_idx\": idx, \"correct\": False, \"mc2_score\": 0, \"mc3_score\": 0})\n",
    "\n",
    "print(f\"End: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ====== RESULTS ======\ntotal = len(results)\ncorrect_count = sum(r[\"correct\"] for r in results)\nwrong_count = total - correct_count\n\n# For vanilla: every wrong answer = hallucination (no verification to catch it)\nhallucination_rate = wrong_count / total * 100\ntruthful_rate = correct_count / total * 100\n\nmc1 = truthful_rate\nmc2 = sum(r[\"mc2_score\"] for r in results) / total * 100\nmc3 = sum(r[\"mc3_score\"] for r in results) / total * 100\n\nprint(\"\\n\" + \"=\"*60)\nprint(f\"RESULTS: {MODEL_NAME} Vanilla (No Verification)\")\nprint(\"=\"*60)\nprint(f\"\\n--- KEY METRICS ---\")\nprint(f\"  Hallucination Rate:  {hallucination_rate:.1f}% ({wrong_count}/{total})\")\nprint(f\"  Truthful Rate:       {truthful_rate:.1f}% ({correct_count}/{total})\")\nprint(f\"  Abstention Rate:     0.0% (vanilla never abstains)\")\nprint(f\"\\n--- MC SCORES ---\")\nprint(f\"  MC1: {mc1:.2f}%\")\nprint(f\"  MC2: {mc2:.2f}%\")\nprint(f\"  MC3: {mc3:.2f}%\")\nprint(\"=\"*60)\n\n# Save\noutput = {\n    \"model\": MODEL_ID,\n    \"model_name\": MODEL_NAME,\n    \"method\": \"Vanilla\",\n    \"total_questions\": total,\n    \"summary\": {\n        \"hallucination_rate\": round(hallucination_rate, 2),\n        \"truthful_rate\": round(truthful_rate, 2),\n        \"abstention_rate\": 0.0,\n    },\n    \"metrics\": {\"mc1\": round(mc1, 2), \"mc2\": round(mc2, 2), \"mc3\": round(mc3, 2)},\n    \"results\": results\n}\n\nwith open(OUTPUT_FILE, \"w\") as f:\n    json.dump(output, f, indent=2)\n    \nprint(f\"\\nSaved to {OUTPUT_FILE}\")\nprint(f\"\\nKey result: {MODEL_NAME} Vanilla has {hallucination_rate:.1f}% hallucination rate\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}