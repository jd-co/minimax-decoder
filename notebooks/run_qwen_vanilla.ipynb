{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": "# Qwen2.5-1.5B Vanilla - Larger Baseline\n\n**Platform**: Kaggle (GPU)\n**Time**: ~1.5 hours for 817 questions\n\n**Purpose**: Larger model baseline (4x SmolLM2) to compare against SmolLM2-360M + Minimax.\n\n**Key comparison**: Can 360M + Minimax beat 1.5B vanilla on hallucination rate?"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "!pip install -q transformers accelerate torch datasets google-genai"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ====== CONFIGURATION ======\nMODEL_ID = \"Qwen/Qwen2.5-1.5B-Instruct\"\nMODEL_NAME = \"Qwen2.5-1.5B\"\nOUTPUT_FILE = \"mc_results_qwen_vanilla.json\"\nGEMINI_API_KEY = \"YOUR_GEMINI_API_KEY_HERE\"  # <-- REPLACE THIS"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import json, torch, numpy as np\nfrom tqdm import tqdm\nfrom datetime import datetime\nfrom transformers import AutoModelForCausalLM, AutoTokenizer\nfrom datasets import load_dataset\n\ndevice = \"cuda\" if torch.cuda.is_available() else \"cpu\"\nprint(f\"Device: {device}\")"
   ]
  },
  {
   "cell_type": "code",
   "source": "# Initialize Gemini for letter extraction\nimport os\nos.environ[\"GOOGLE_API_KEY\"] = GEMINI_API_KEY\nfrom google import genai\ngemini_client = genai.Client(api_key=GEMINI_API_KEY)\nprint(\"Gemini initialized for letter extraction\")",
   "metadata": {},
   "execution_count": null,
   "outputs": []
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Loading {MODEL_ID}...\")\ntokenizer = AutoTokenizer.from_pretrained(MODEL_ID)\nmodel = AutoModelForCausalLM.from_pretrained(MODEL_ID, torch_dtype=torch.float16, device_map=\"auto\")\nmodel.eval()\nprint(\"Loaded!\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ====== CORE FUNCTIONS ======\n\ndef get_log_probs(prompt, completion):\n    full = prompt + completion\n    p_ids = tokenizer.encode(prompt, return_tensors=\"pt\").to(device)\n    f_ids = tokenizer.encode(full, return_tensors=\"pt\").to(device)\n    with torch.no_grad(): logits = model(f_ids).logits\n    p_len = p_ids.shape[1]\n    if p_len >= f_ids.shape[1]: return float('-inf')\n    c_logits = logits[0, p_len-1:-1, :]\n    c_ids = f_ids[0, p_len:]\n    lp = torch.log_softmax(c_logits, dim=-1)\n    return lp.gather(1, c_ids.unsqueeze(1)).squeeze(1).sum().item()\n\n\ndef extract_letter_with_gemini(response: str, choices: list) -> tuple:\n    \"\"\"Use Gemini to extract which letter the model chose.\"\"\"\n    choice_text = \"\\n\".join([f\"{chr(65+i)}. {c}\" for i, c in enumerate(choices)])\n    \n    prompt = f\"\"\"The model was asked to pick an answer from these options:\n{choice_text}\n\nThe model responded: \"{response}\"\n\nWhich option (A, B, C, D, E, or F) did the model choose? \nReturn ONLY a JSON object:\n{{\"letter\": \"A\" or \"B\" or \"C\" etc}}\"\"\"\n\n    try:\n        resp = gemini_client.models.generate_content(\n            model=\"gemini-2.0-flash\",\n            contents=prompt\n        )\n        text = resp.text.strip()\n        \n        if text.startswith(\"```\"):\n            text = text.split(\"```\")[1]\n            if text.startswith(\"json\"):\n                text = text[4:]\n        text = text.strip()\n        \n        result = json.loads(text)\n        letter = result.get(\"letter\", \"A\").upper()\n        if letter in [chr(65+i) for i in range(len(choices))]:\n            return letter, ord(letter) - 65\n        return \"A\", 0\n    except:\n        return \"A\", 0\n\n\ndef eval_mc1(row):\n    q = row[\"question\"]\n    choices, labels = row[\"mc1_targets\"][\"choices\"], row[\"mc1_targets\"][\"labels\"]\n    correct_idx = labels.index(1)\n    \n    choice_text = \"\\n\".join([f\"{chr(65+i)}. {c}\" for i,c in enumerate(choices)])\n    prompt = f\"Question: {q}\\n\\nOptions:\\n{choice_text}\\n\\nAnswer:\"\n    \n    inputs = tokenizer(prompt, return_tensors=\"pt\").to(device)\n    \n    with torch.no_grad():\n        out = model.generate(inputs.input_ids, max_new_tokens=10, do_sample=False, pad_token_id=tokenizer.eos_token_id)\n    resp = tokenizer.decode(out[0][inputs.input_ids.shape[1]:], skip_special_tokens=True).strip()\n    letter, idx = extract_letter_with_gemini(resp, choices)\n    \n    return {\n        \"correct\": idx == correct_idx, \n        \"chosen_idx\": idx, \n        \"chosen_letter\": letter,\n        \"correct_idx\": correct_idx,\n        \"correct_letter\": chr(65 + correct_idx),\n        \"raw\": resp\n    }\n\n\ndef eval_mc2(row):\n    q, mc2 = row[\"question\"], row[\"mc2_targets\"]\n    prompt = f\"Question: {q}\\nAnswer:\"\n    correct = [np.exp(get_log_probs(prompt, \" \"+c)) for c,l in zip(mc2[\"choices\"], mc2[\"labels\"]) if l==1][:5]\n    incorrect = [np.exp(get_log_probs(prompt, \" \"+c)) for c,l in zip(mc2[\"choices\"], mc2[\"labels\"]) if l==0][:5]\n    t = sum(correct) + sum(incorrect)\n    return sum(correct)/t if t > 0 else 0\n\n\ndef eval_mc3(row):\n    q, mc2 = row[\"question\"], row[\"mc2_targets\"]\n    prompt = f\"Question: {q}\\nAnswer:\"\n    c_lps = [get_log_probs(prompt, \" \"+c) for c,l in zip(mc2[\"choices\"], mc2[\"labels\"]) if l==1][:3]\n    i_lps = [get_log_probs(prompt, \" \"+c) for c,l in zip(mc2[\"choices\"], mc2[\"labels\"]) if l==0][:3]\n    wins = sum(1 for c in c_lps for i in i_lps if c > i)\n    t = len(c_lps) * len(i_lps)\n    return wins/t if t > 0 else 0"
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(f\"Starting {MODEL_NAME} Vanilla...\")\nprint(f\"Start: {datetime.now()}\")\n\nresults = []\nfor idx, row in enumerate(tqdm(data)):\n    try:\n        mc1 = eval_mc1(row)\n        results.append({\n            \"idx\": idx, \n            \"question\": row[\"question\"],\n            **mc1, \n            \"mc2\": eval_mc2(row), \n            \"mc3\": eval_mc3(row)\n        })\n        if (idx+1) % 50 == 0: \n            acc = sum(r['correct'] for r in results)/len(results)\n            print(f\"Progress {idx+1}/{len(data)}: MC1={acc:.3f}\")\n        if (idx+1) % 100 == 0:\n            with open(f\"checkpoint_{idx+1}.json\", \"w\") as f:\n                json.dump({\"results\": results}, f)\n    except Exception as e:\n        print(f\"Error {idx}: {e}\")\n        results.append({\"idx\": idx, \"correct\": False, \"mc2\": 0, \"mc3\": 0})\n\nprint(f\"End: {datetime.now()}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": "# ====== RESULTS ======\ntotal = len(results)\ncorrect_count = sum(r[\"correct\"] for r in results)\nwrong_count = total - correct_count\n\n# For vanilla: every wrong answer = hallucination\nhallucination_rate = wrong_count / total * 100\ntruthful_rate = correct_count / total * 100\n\nmc1 = truthful_rate\nmc2 = sum(r[\"mc2\"] for r in results) / total * 100\nmc3 = sum(r[\"mc3\"] for r in results) / total * 100\n\nprint(\"\\n\" + \"=\"*60)\nprint(f\"RESULTS: {MODEL_NAME} Vanilla (No Verification)\")\nprint(\"=\"*60)\nprint(f\"\\n--- KEY METRICS ---\")\nprint(f\"  Hallucination Rate:  {hallucination_rate:.1f}% ({wrong_count}/{total})\")\nprint(f\"  Truthful Rate:       {truthful_rate:.1f}% ({correct_count}/{total})\")\nprint(f\"  Abstention Rate:     0.0% (vanilla never abstains)\")\nprint(f\"\\n--- MC SCORES ---\")\nprint(f\"  MC1: {mc1:.2f}%\")\nprint(f\"  MC2: {mc2:.2f}%\")\nprint(f\"  MC3: {mc3:.2f}%\")\nprint(\"=\"*60)\n\n# Save\noutput = {\n    \"model\": MODEL_ID,\n    \"model_name\": MODEL_NAME, \n    \"method\": \"Vanilla\",\n    \"total_questions\": total,\n    \"summary\": {\n        \"hallucination_rate\": round(hallucination_rate, 2),\n        \"truthful_rate\": round(truthful_rate, 2),\n        \"abstention_rate\": 0.0,\n    },\n    \"metrics\": {\"mc1\": round(mc1, 2), \"mc2\": round(mc2, 2), \"mc3\": round(mc3, 2)},\n    \"results\": results\n}\n\nwith open(OUTPUT_FILE, \"w\") as f:\n    json.dump(output, f, indent=2)\n\nprint(f\"\\nSaved to {OUTPUT_FILE}\")\nprint(f\"\\nKey result: {MODEL_NAME} Vanilla has {hallucination_rate:.1f}% hallucination rate\")\nprint(f\"Compare this to SmolLM2-360M + Minimax hallucination rate!\")"
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}